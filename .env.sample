#DATA_DIR=./data
# following foler contains data after Aug 10, rememeber change BASIC_CSV also
DATA_DIR=option/put/unprocessed2 
# following foler contains main train data before Aug 10
#DATA_DIR=option/put/unprocessed
GLOB=coveredPut_*.csv
TARGET_TIME=11:00
BATCH_SIZE=30
ALL_SNAPSHOTS=0
REFRESH_CACHE=0
CACHE_DIR=output
OUTPUT_DIR=./output

# before Aug 10
BASIC_CSV=trades_raw_t1.csv

#=== a01 For merge with gex
GEX_BASE_DIR=gex101
GEX_TARGET_TIME=11:00
#GEX_CSV=labeled_trades_with_gex_normal.csv

#=== a02  For macro feature
#PX_DIR=output/price_cache
VIX_CSV=output/vix_data.csv
PX_BASE_DIR=output/price_cache
MACRO_FEATURE_CSV=trades_with_gex_macro_t1.csv
# === a03merge_fundamentals_events ===
#TRADES_CSV=output/labeled_trades_normal_gex_macro3.csv
EARNINGS_CSV=output/symbol_info/earnings_calendar.csv
#OUTPUT_CSV=output/labeled_trades_enriched.csv
# === a09lable_data===
OUTPUT_CSV=labeled_trades_t1a.csv

#######tails loss model, copied from train_tail_with_gex_sample.env
# ========= Inputs / Outputs =========
#CSV_INPUT=output/labeled_trades_with_gex.csv
TAIL_OUT_DIR = "output/tails_train/v6b_we"
TAIL_MODEL_OUT=tail_model_gex_v6b_ne_cut05.pkl
TAIL_IMP_OUT=tail_gex_v6b_ne_cut05_feature_importances.csv
TAIL_SCORES_OUT=tail_gex_v6b_ne_cut05_scores_oof.csv
TAIL_METRICS_OUT=tail_train_metrics_v6b_ne_cut05.json
SAVE_SCORES_SAMPLE=40000
# or raw, or annualized, or per_month
LABEL_ON=per_month

# ========= Labeling =========
# Tail fraction by *dollar PnL* quantile (e.g., 0.03 = worst 3%)
TAIL_PCT=0.05

# ========= Cross-Validation =========
# CV_TYPE: stratified | time
CV_TYPE=stratified
FOLDS=8
# ========= Reproducibility =========
SEED=42
# tails gate training
OUT_DIR=output






####tail score
# Scoring config (.env)
#CSV_INPUT=/mnt/data/labeled_trades_with_gex.csv
#MODEL_IN=models/tail_model_gex_v1b.pkl
TAIL_SCORE_INPUT=output/labeled_trades_normal_enriched.csv
#TAIL_MODEL_IN=models/tail_model_gex_v3lean_cut05.pkl
TAIL_SCORE_OUT=output/tails_score/scored_with_tail_pct_mon_cut5_new.csv
TAIL_KEEP_PROBA_COL = tail_proba
TAIL_PRED_COL = is_tail_preda
# thresh hold data for cut 5 from result

# Following is for training
#Target Recall	Threshold	Actual Recall	Precision	Keep Rate
#0.83	        |0.063	     |0.830	        |0.212	     |80.6%
#0.90	        |0.043	     |0.900	        |0.175	     |74.4%
#0.92	        |0.036	     |0.920	        |0.161	     |71.6%
#0.96	        |0.020	     |0.960	        |0.129	     |63.1%

TAIL_THRESHOLD = 0.030
# Fixed threshold chosen for ~35% precision (OOF):
#OUTPUT_PATH=tail_gex_p40_scores.csv

# Optional alternatives:
# THRESHOLD_FROM=best_f1
# METRICS_IN=/mnt/data/tail_train_metrics_v1.json


##### train_winner_classifier, pct version

# === Required ===
WINNER_INPUT=output/labeled_trades_enriched.csv
WINNER_OUTPUT_DIR=output/winner_train/v6_oof_ne_straited_w_lgbm
#WINNER_OUTPUT_DIR=output/winner_train/v6_oof_ne_straited
WINNER_MODEL_NAME=winner_classifier_v6_oof_ne_w
# options are return_mon, return_ann, return_pct
WINNER_TRAIN_TARGET=return_mon

WINNER_MODEL_TYPE=lgbm
#WINNER_MODEL_TYPE=catboost
#WINNER_MODEL_TYPE=rf

# === Optional ===
WINNER_FEATURES=
WINNER_ID_COLS=symbol,tradeTime,return_pct,return_mon,return_ann,daysToExpiration

WINNER_TEST_SIZE=0.3
WINNER_RANDOM_STATE=42
WINNER_CLASSIFIER_N_ESTIMATORS=400
WINNER_CLASS_WEIGHT=balanced_subsample
WINNER_MAX_DEPTH=
WINNER_MIN_SAMPLES_LEAF=1
WINNER_MIN_SAMPLES_SPLIT=2

# Missing value strategy
# 1=median impute (on train only); 0=drop rows with NaNs
WINNER_IMPUTE_MISSING=1

# Sample weights tied to |return_pct|
WINNER_USE_WEIGHTS=1
WINNER_WEIGHT_ALPHA=0.02
WINNER_WEIGHT_MIN=0.5
WINNER_WEIGHT_MAX=10.0

# Threshold targets for reporting (can be comma separated or JSON)
WINNER_TARGET_RECALL=
WINNER_TARGET_PRECISION=0.88,0.92

# new added for OOF training
WINNER_OOF_FOLDS=5
# 'auto' (use TimeSeries if trade_date present), '1' to force TimeSeries, '0' to force Stratified
WINNER_TIME_SERIES=0  

####### ===== Winner scoring =====
WINNER_SCORE_INPUT=output/labeled_trades_t1.csv
WINNER_MODEL_IN=output/winner_train/external/winner_classifier_v6_oof_ne_w_lgbm.pkl
WINNER_SCORE_OUT_FOLDER=output/winner_score/rfctr_lgbm
WINNER_SCORE_OUT=scores_winner_lgbm_t1.csv
WINNER_PROBA_COL=win_proba
WINNER_PRED_COL=win_predict
# For splitting train and test generated in training, in the folder WINNER_OUTPUT_DIR 
WINNER_SPLIT_FILE=winner_scores_split.csv
WINNER_OOF_FILE=


# Threshold policy

# fixed numeric override
WINNER_SCORE_THRESHOLD=
# use best F1 from training pack if no fixed threshold and no auto-calibration
WINNER_SCORE_USE_PACK_BEST_F1=1
# if AUTO_CALIBRATE=1 and labels exist (return_pct), pick lowest threshold achieving >= this
WINNER_SCORE_TARGET_PRECISION=0.90 
WINNER_SCORE_TARGET_RECALL=
# set to 1 only when scored CSV includes labels and  want data-driven threshold
WINNER_SCORE_AUTO_CALIBRATE=1

################Evaluate
# === Required ===
# Evaluate the tail model
#EVAL_INPUT=output/tails_score/scored_with_tail_pct_mon_cut5.csv
EVAL_INPUT=output/tails_score/scored_with_tail_pct_mon_cut5_new.csv
#EVAL_INPUT=output/tails_score/tail_gex_p35_scores_y2.csv
EVAL_OUTPUT_DIR=output/eval/tail_scored/


# Evaluate the winner classifer training dataset (the test set)
#EVAL_INPUT=output/winner_train/v4/winner_scores_split.csv
#EVAL_OUTPUT_DIR=output/eval/win/

# Evaluate the scored training dataset (the test set)
#EVAL_INPUT=output/winner_score/scores_winner.csv
#EVAL_OUTPUT_DIR=output/eval/win_scored/

# === Probabilities & thresholds ===
EVAL_PROBA_COL=tail_proba
#EVAL_PROBA_COL=proba
#EVAL_PROBA_COL=score
# Fixed thresholds (optional)
EVAL_FIXED_THRESHOLDS=
# Pick thresholds by targets (optional)

# For tail model, use target recall
EVAL_TARGET_RECALL=0.86,0.92
# For winner model, use target precision
#EVAL_TARGET_PRECISION=0.90,0.95,0.98

# === Label mode ===
# winner  -> label = (return_pct > 0)
# tail_pct -> label = (return_pct <= quantile(K))
# tail_pnl -> label = (total_pnl <= quantile(K))
# provided -> label from EVAL_LABEL_COL
EVAL_LABEL_MODE=winner
#EVAL_LABEL_MODE=tail_pct
EVAL_TAIL_K=0.05
EVAL_LABEL_COL=
EVAL_RETURN_COL=return_mon
#EVAL_PNL_COL=total_pnl

EVAL_SPLIT_COL=is_train

# === Optional filtering/grouping ===
# Example: only evaluate short-dated trades
EVAL_FILTER_QUERY=
# Example: compute AUC/PR by dte bucket
EVAL_GROUP_COLS=daysToExpiration



SYMBOL_COL=baseSymbol
TRADE_DATE_COL=trade_date
DROP_DUPLICATES=true
STRICT_LENGTH_CHECK=true


#== Rescue model
RESCUE_OUT=output/rescue_tail/v2d