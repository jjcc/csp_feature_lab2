#DATA_DIR=./data
DATA_DIR = W:/dev/data/option_data101/put
GLOB=coveredPut_*.csv
TARGET_TIME=11:00
BATCH_SIZE=30
ALL_SNAPSHOTS=0
REFRESH_CACHE=0
CACHE_DIR=./output
OUTPUT_DIR=./output
BASIC_CSV=labeled_trades_normal.csv
GEX_CSV=labeled_trades_normal_gex.csv
MACRO_FEATURE_CSV=labeled_trades_normal_gex_macro.csv

#For merge with gex
GEX_BASE_DIR=W:\dev\data\gex101\processed\csv
GEX_TARGET_TIME=11:00
LABELED_TRADES_WITH_GEX = labeled_trades_with_gex_normal.csv

# extra in tails training
PX_DIR=output/price_cache
VIX_CSV=output/vix_data.csv


##### winner classifier model
# Path to your labeled trades CSV (must contain columns: win, return_pct; features as configured)
CSV=output/labeled_trades_normal.csv

OUTPUT_DIR_W=output/winner
# Features leave blank to auto-pick defaults (no GEX)
FEATURES=
USE_WEIGHTED_CLASSIFIER=true
TEST_SIZE=0.30
CLASSIFIER_N_ESTIMATORS=500






# Comma-separated feature list; leave blank to auto-pick defaults present in the CSV
#FEATURES=moneyness,percentToBreakEvenBid,impliedVolatilityRank1y,delta,potentialReturn,potentialReturnAnnual,breakEvenProbability,openInterest,volume,underlyingLastPrice,strike

# Use weighted classifier (weights ~ 1 + 0.02*|return_pct|, clipped to [0.5, 10])
USE_WEIGHTED_CLASSIFIER=true

# Train/validation split

# Model sizes
CLASSIFIER_N_ESTIMATORS=500
REGRESSOR_N_ESTIMATORS=600

# Optional hybrid ranking gates
TAIL_PROBA_MAX=0.35
MIN_WIN_PROBA=0.50


######## for dicile lift
# Input / output
# Path to your labeled trades CSV (must contain columns: win, return_pct; features as configured)
#CSV=output/labeled_trades.csv
OUTPUT_DIR_DECILES=output/deciles

# Train/test + model sizes (bump these on your machine)
TEST_SIZE=0.30
SAMPLE_N=80000
N_EST_REG=500
N_EST_CLS=500

# Hybrid gates and sim params
USE_TAIL_FILTER=true
TAIL_PROBA_MAX=0.35
MIN_WIN_PROBA=0.60
INITIAL_CASH=100000
MAX_CONCURRENT=5


#######tails loss model, copied from train_tail_with_gex_sample.env
# ========= Inputs / Outputs =========
CSV_INPUT=output/labeled_trades_with_gex.csv
MODEL_OUT=models/tail_model_gex_v2_cut05.pkl
IMP_OUT=output/tails_train/tail_gex_v2_cut05_feature_importances.csv
SCORES_OUT=output/tails_train/tail_gex_v2_cut05_scores_oof.csv
METRICS_OUT=output/tails_train/tail_train_metrics_v2_cut05.json
SAVE_SCORES_SAMPLE=20000

# ========= Labeling =========
# Tail fraction by *dollar PnL* quantile (e.g., 0.03 = worst 3%)
TAIL_PCT=0.05

# ========= Cross-Validation =========
# CV_TYPE: stratified | time
CV_TYPE=stratified
FOLDS=5

# ========= Reproducibility =========
SEED=42




####tail score
# Scoring config (.env)
#CSV_INPUT=/mnt/data/labeled_trades_with_gex.csv
#MODEL_IN=models/tail_model_gex_v1b.pkl
TAIL_SCORE_INPUT=output/labeled_trades_with_gex.csv
TAIL_MODEL_IN=models/tail_model_gex_v2_cut05.pkl
TAIL_SCORE_OUT=output/tails_score/scored_with_tail_pct_cut5.csv
TAIL_KEEP_PROBA_COL = tail_proba
TAIL_PRED_COL = is_tail_preda
# thresh hold data for cut 5 from result
#Target Recall	Threshold	Actual Recall	Precision	Keep Rate
#65%	        | 0.10701	| 65.03%	    |28.83%	    |88.78%
#83%	        | 0.04655	| 83.02%	    |16.87%	    |75.53%
#90%	        | 0.03082	| 90.05%	    |12.74%	    |64.83%
TAIL_THRESHOLD = 0.03082
# Fixed threshold chosen for ~35% precision (OOF):
#OUTPUT_PATH=tail_gex_p40_scores.csv

# Optional alternatives:
# THRESHOLD_FROM=best_f1
# METRICS_IN=/mnt/data/tail_train_metrics_v1.json


##### train_winner_classifier, pct version

# === Required ===
WINNER_INPUT=output/labeled_trades_normal.csv
WINNER_OUTPUT_DIR=output/winner

# === Optional ===
WINNER_FEATURES=moneyness,percentToBreakEvenBid,impliedVolatilityRank1y,delta,potentialReturn,potentialReturnAnnual,breakEvenProbability,openInterest,volume,underlyingLastPrice,strike     
WINNER_ID_COLS=symbol,tradeTime,return_pct,daysToExpiration

WINNER_TEST_SIZE=0.3
WINNER_RANDOM_STATE=42
WINNER_CLASSIFIER_N_ESTIMATORS=400
WINNER_CLASS_WEIGHT=balanced_subsample
WINNER_MAX_DEPTH=
WINNER_MIN_SAMPLES_LEAF=1
WINNER_MIN_SAMPLES_SPLIT=2

# Missing value strategy
# 1=median impute (on train only); 0=drop rows with NaNs
WINNER_IMPUTE_MISSING=1

# Sample weights tied to |return_pct|
WINNER_USE_WEIGHTS=1
WINNER_WEIGHT_ALPHA=0.02
WINNER_WEIGHT_MIN=0.5
WINNER_WEIGHT_MAX=10.0

# Threshold targets for reporting (can be comma separated or JSON)
WINNER_TARGET_RECALL=
WINNER_TARGET_PRECISION=0.9,0.95,0.98


####### ===== Winner scoring =====
WINNER_SCORE_INPUT=output/labeled_trades_normal.csv
WINNER_MODEL_IN=output/winner/model_pack.pkl
WINNER_SCORE_OUT=output/winner_score/scores_winner.csv
WINNER_PROBA_COL=proba
WINNER_PRED_COL=win
# For splitting train and test
WINNER_SPLIT_FILE=output/winner/winner_scores_split.csv


# Threshold policy

# fixed numeric override
WINNER_SCORE_THRESHOLD=
# use best F1 from training pack if no fixed threshold and no auto-calibration
WINNER_SCORE_USE_PACK_BEST_F1=1
# if AUTO_CALIBRATE=1 and labels exist (return_pct), pick lowest threshold achieving >= this
WINNER_SCORE_TARGET_PRECISION=0.95 
WINNER_SCORE_TARGET_RECALL=
# set to 1 only when your scored CSV includes labels and you want data-driven threshold
WINNER_SCORE_AUTO_CALIBRATE=1

################Evaluate
# === Required ===
# Evaluate the tail model
#EVAL_INPUT=output/tails_score/scored_with_tail_pct_cut5_b.csv
#EVAL_OUTPUT_DIR=output/eval/

# Evaluate the training dataset (the test set)
EVAL_INPUT=output/winner/winner_scores_split.csv
EVAL_OUTPUT_DIR=output/eval/win/

# Evaluate the score output
#EVAL_INPUT=output/winner_score/scores_winner.csv
#EVAL_OUTPUT_DIR=output/eval/win_scored/

# === Probabilities & thresholds ===
#EVAL_PROBA_COL=tail_proba
EVAL_PROBA_COL=proba
# Fixed thresholds (optional)
EVAL_FIXED_THRESHOLDS=
# Pick thresholds by targets (optional)

# For tail model, use target recall
#EVAL_TARGET_RECALL=0.65,0.83,0.90
# For winner model, use target precision
EVAL_TARGET_PRECISION=0.90,0.95,0.98

# === Label mode ===
# winner  -> label = (return_pct > 0)
# tail_pct -> label = (return_pct <= quantile(K))
# tail_pnl -> label = (total_pnl <= quantile(K))
# provided -> label from EVAL_LABEL_COL
EVAL_LABEL_MODE=winner
#EVAL_LABEL_MODE=tail_pct
EVAL_TAIL_K=0.05
EVAL_LABEL_COL=
EVAL_RETURN_COL=return_pct
EVAL_PNL_COL=total_pnl

EVAL_SPLIT_COL=is_train

# === Optional filtering/grouping ===
# Example: only evaluate short-dated trades
EVAL_FILTER_QUERY=
# Example: compute AUC/PR by dte bucket
EVAL_GROUP_COLS=daysToExpiration